{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/juanhevia/IDIL\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "INPUT_EXPERTS_NAME = \"CleanupSingle-v0_100.pkl\"\n",
    "EXPERTS_PATH = \"idil_train/experts/\"\n",
    "\n",
    "def load_experts(input_experts):\n",
    "    with open(os.path.join(EXPERTS_PATH, input_experts), \"rb\") as f:\n",
    "        experts = pkl.load(f)\n",
    "\n",
    "    return experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_experts = load_experts(INPUT_EXPERTS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gotten object: <class 'dict'> for CleanupSingle-v0_100.pkl\n",
      "Keys are: dict_keys(['states', 'next_states', 'actions', 'latents', 'rewards', 'dones', 'lengths'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gotten object: {type(original_experts)} for {INPUT_EXPERTS_NAME}\")\n",
    "print(f\"Keys are: {original_experts.keys() if hasattr(original_experts, 'keys') else 'No keys'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gotten 100 expert trajectories\n",
      "Key: states has length: 100\n",
      "Key: next_states has length: 100\n",
      "Key: actions has length: 100\n",
      "Key: latents has length: 100\n",
      "Key: rewards has length: 100\n",
      "Key: dones has length: 100\n",
      "Key: lengths has length: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gotten {len(original_experts['states'])} expert trajectories\")\n",
    "\n",
    "for key in original_experts.keys():\n",
    "    print(f\"Key: {key} has length: {len(original_experts[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new train and test sets\n",
    "IDX_SPLIT = int(len(original_experts['states']) * 0.7)\n",
    "\n",
    "train_experts = {key: original_experts[key][:IDX_SPLIT] for key in original_experts.keys()}\n",
    "test_experts = {key: original_experts[key][IDX_SPLIT:] for key in original_experts.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 70 expert trajectories\n",
      "Key: states has length: 70\n",
      "Key: next_states has length: 70\n",
      "Key: actions has length: 70\n",
      "Key: latents has length: 70\n",
      "Key: rewards has length: 70\n",
      "Key: dones has length: 70\n",
      "Key: lengths has length: 70\n",
      "------------------------------\n",
      "Test set has 30 expert trajectories\n",
      "Key: states has length: 30\n",
      "Key: next_states has length: 30\n",
      "Key: actions has length: 30\n",
      "Key: latents has length: 30\n",
      "Key: rewards has length: 30\n",
      "Key: dones has length: 30\n",
      "Key: lengths has length: 30\n"
     ]
    }
   ],
   "source": [
    "# assess new lengths for each key\n",
    "print(f\"Train set has {len(train_experts['states'])} expert trajectories\")\n",
    "\n",
    "for key in train_experts:\n",
    "    print(f\"Key: {key} has length: {len(train_experts[key])}\")\n",
    "\n",
    "print(\"---\"*10)\n",
    "\n",
    "print(f\"Test set has {len(test_experts['states'])} expert trajectories\")\n",
    "for key in test_experts:\n",
    "    print(f\"Key: {key} has length: {len(test_experts[key])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store train split in a new file\n",
    "new_train_file = INPUT_EXPERTS_NAME.replace(\".pkl\", f\"_train{IDX_SPLIT}.pkl\")\n",
    "new_test_file = INPUT_EXPERTS_NAME.replace(\".pkl\", f\"_test{IDX_SPLIT}.pkl\")\n",
    "\n",
    "TEST_PATH = \"idil_train/test_data/\"\n",
    "\n",
    "# save train file\n",
    "with open(os.path.join(EXPERTS_PATH, new_train_file), \"wb\") as f:\n",
    "    pkl.dump(train_experts, f)\n",
    "\n",
    "# save test file to test_data\n",
    "with open(os.path.join(TEST_PATH, new_test_file), \"wb\") as f:\n",
    "    pkl.dump(test_experts, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aidil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
